{
  "note": "# MCP Server-Side LLM Configuration Architecture\n\n## Overview\n\nImplemented a comprehensive LLM provider system that allows the MCP server to configure and manage multiple LLM providers (OpenRouter, OpenAI, Ollama) with secure API key storage using Bun.secrets. The key architectural decision was making the MCP server handle both configuration prompting AND storage, avoiding cross-process credential sharing issues.\n\n## Core Components\n\n### 1. API Key Manager (`/src/core-mcp/services/api-key-manager.ts`)\n- **Purpose**: Secure API key storage using Bun.secrets exclusively\n- **Key Methods**: `storeApiKey()`, `getApiKey()`, `deleteApiKey()`, `listStoredProviders()`\n- **Storage Location**: OS keychain (Keychain Services/GNOME Keyring/Windows Credential Manager)\n- **Critical Decision**: No fallback storage - requires Bun runtime or feature is unavailable\n\n### 2. MCP LLM Configurator (`/src/core-mcp/services/mcp-llm-configurator.ts`)\n- **Purpose**: Handles automatic LLM configuration loading with priority system\n- **Configuration Priority**: Config file (.a24z/llm-config.json) → Stored API keys (automatic provider selection)\n- **Provider Priority**: ['openrouter', 'openai', 'anthropic', 'ollama'] \n- **Key Method**: `ensureLLMConfiguration()` - loads best available config on server startup\n\n### 3. Configure LLM Tool (`/src/core-mcp/tools/ConfigureLLMTool.ts`)\n- **Purpose**: Interactive LLM provider management through MCP\n- **Actions**: list, status, configure, test, remove\n- **Key Feature**: Shows provider status with transparency about configuration source\n- **Schema**: Supports all provider-specific fields (siteUrl, siteName for OpenRouter)\n\n## Integration Flow\n\n### Server Startup\n1. McpServer constructor calls `setupDefaultTools()` → adds ConfigureLLMTool\n2. `start()` method calls `initializeLLMService()`\n3. `ensureLLMConfiguration()` loads config using priority system\n4. Server logs show active provider and configuration source for transparency\n\n### Configuration Process\n1. User calls ConfigureLLMTool via MCP client (Claude Desktop)\n2. Tool validates provider and stores credentials using ApiKeyManager\n3. Credentials persist in OS keychain across server restarts\n4. Next startup automatically loads stored configuration\n\n## Provider Selection Logic\n\nWhen multiple providers are configured:\n- **Automatic Selection**: Uses provider priority order\n- **Transparency**: Server startup logs show which provider is active and why\n- **Override Options**: Config file can force specific provider\n- **Status Visibility**: `configure_llm` tool with \"status\" action shows all configured providers\n\n## Cross-Process Challenge Solved\n\n**Original Problem**: VSCode extension and MCP server run as separate processes - keychain access might not be shared across process boundaries.\n\n**Solution**: Moved configuration responsibility entirely to MCP server:\n- Server handles user prompting through ConfigureLLMTool\n- Server manages storage using its own process context\n- No cross-process credential sharing required\n- VSCode extension can trigger configuration through MCP calls\n\n## Key Technical Decisions\n\n### No Environment Variable Support\n- **Decision**: Library does not support environment variables for configuration\n- **Rationale**: Consistent with library's design philosophy\n- **Configuration Methods**: 1) .a24z/llm-config.json file 2) Stored API keys via MCP tools\n\n### Bun.secrets Exclusive\n- **Decision**: No fallback storage mechanisms (rejected .env.local)\n- **Rationale**: User explicitly required secure-only storage\n- **Impact**: Feature unavailable on non-Bun environments\n\n### Server-Side Configuration\n- **Decision**: MCP server handles both prompting and storage\n- **Rationale**: Avoids cross-process keychain access issues\n- **Impact**: Configuration survives server restarts, works across all MCP clients\n\n### Provider Priority System\n- **Decision**: Automatic provider selection with transparent logging\n- **Rationale**: User shouldn't need to manually specify which provider to use\n- **Impact**: System \"just works\" when multiple providers configured\n\n## Files Modified/Created\n\n- **NEW**: `api-key-manager.ts` - Bun.secrets integration\n- **NEW**: `mcp-llm-configurator.ts` - Configuration management\n- **NEW**: `ConfigureLLMTool.ts` - Interactive configuration\n- **MODIFIED**: `llm-service.ts` - Added OpenRouter support, async loadConfig()\n- **MODIFIED**: `McpServer.ts` - Added LLM service initialization\n- **MODIFIED**: `lib.ts` - Exported new components\n\n## Configuration Methods\n\n### 1. Repository Config File (.a24z/llm-config.json)\n```json\n{\n  \"provider\": \"openrouter\",\n  \"model\": \"meta-llama/llama-3.2-3b-instruct\",\n  \"temperature\": 0.3\n}\n```\n\n### 2. MCP Tool Configuration\n```json\n{\n  \"action\": \"configure\",\n  \"provider\": \"openrouter\", \n  \"apiKey\": \"sk-or-v1-...\",\n  \"model\": \"meta-llama/llama-3.2-3b-instruct\"\n}\n```\n\n## Testing Considerations\n\n- **Cross-Process Testing**: Verify keychain access works in MCP server context\n- **Priority Logic**: Test configuration loading with multiple sources present\n- **Error Handling**: Validate behavior when Bun unavailable or API keys invalid\n- **Provider Switching**: Test behavior when changing active provider",
  "anchors": [
    "src/core-mcp/services",
    "src/core-mcp/tools/ConfigureLLMTool.ts",
    "src/core-mcp/server/McpServer.ts"
  ],
  "tags": [
    "mcp",
    "architecture",
    "implementation",
    "llm-integration",
    "security",
    "provider-management"
  ],
  "confidence": "high",
  "type": "decision",
  "metadata": {
    "toolVersion": "2.0.0",
    "createdBy": "create_repository_note_tool"
  },
  "id": "note-1756159495420-o9kzhes23",
  "timestamp": 1756159495420
}
